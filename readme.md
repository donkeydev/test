üé• Roteiro para Apresenta√ß√£o em V√≠deo: Solu√ß√£o Web Crawler AxurEste roteiro descreve a arquitetura e as decis√µes t√©cnicas tomadas para construir um Web Crawler ass√≠ncrono e de alto desempenho, atendendo a todos os requisitos do teste t√©cnico.1. Vis√£o Geral e Requisitos AtendidosObjetivo Central: Construir um servi√ßo API que inicie buscas por palavras-chave (keywords) em um website (BASE_URL) de forma ass√≠ncrona, retornando resultados parciais em tempo real.Requisitos Chave Atendidos:API HTTP (R1): Endpoints POST /crawl (iniciar) e GET /crawl/:id (consultar).Assincronicidade (R5, R6): O POST retorna instantaneamente, e o crawling roda em segundo plano, permitindo consultas parciais (status: active).Performance: Uso de Java's HttpClient com I/O N√£o-Bloqueante para garantir alta velocidade na descoberta de URLs.Robustez e Concorr√™ncia: Uso de estruturas thread-safe para lidar com m√∫ltiplas buscas simult√¢neas.2. Arquitetura da Solu√ß√£o (Divis√£o de Responsabilidades)A solu√ß√£o est√° dividida em tr√™s componentes principais, seguindo o princ√≠pio de Separa√ß√£o de Preocupa√ß√µes:2.1. Main.java (Servidor API e Inicializa√ß√£o da Performance)Fun√ß√£o: Ponto de entrada da aplica√ß√£o, respons√°vel por configurar o servidor HTTP (Spark) e inicializar recursos cr√≠ticos de performance.Decis√£o T√©cnica Chave (Otimiza√ß√£o de I/O):O HttpClient (SHARED_HTTP_CLIENT) √© inicializado apenas uma vez e √© compartilhado entre todas as requisi√ß√µes (Singleton Pattern). Isso elimina a lat√™ncia de inicializa√ß√£o de sockets e pools de threads a cada nova busca.Foi injetado um ExecutorService dedicado (Executors.newFixedThreadPool(10)) no HttpClient. Essa abordagem for√ßa o I/O a usar um pool otimizado, reduzindo drasticamente a lat√™ncia da primeira requisi√ß√£o (de > 1900 ms para ~500 ms).Implica√ß√£o: A inje√ß√£o de depend√™ncia do HttpClient garante performance e facilita testes de unidade (embora n√£o implementados aqui).2.2. CrawlManager.java (Gerenciador de Jobs e Concorr√™ncia)Fun√ß√£o: Controla o ciclo de vida das buscas (cria√ß√£o e consulta) e gerencia a concorr√™ncia entre elas.Decis√µes T√©cnicas Chave:Armazenamento Thread-Safe: Utiliza um Map<String, CrawlJob> implementado com ConcurrentHashMap (allJobs) para armazenar os resultados. Isso garante que m√∫ltiplos threads do crawler (escrevendo URLs) e o thread da API (lendo resultados) possam acessar e modificar os dados de forma segura e eficiente, sem bloqueios desnecess√°rios.In√≠cio Ass√≠ncrono: O m√©todo startNewCrawl usa um jobExecutorPool (newCachedThreadPool) para delegar a execu√ß√£o do webCrawler.startCrawl(job) a uma thread separada.Implica√ß√£o: Essa delega√ß√£o garante que a API (POST /crawl) responda imediatamente com o ID do job (Requisito 5 e 1.a), enquanto o trabalho pesado √© realizado em segundo plano, mantendo a API responsiva.2.3. WebCrawler.java (A Intelig√™ncia do Crawling N√£o-Bloqueante)Fun√ß√£o: Respons√°vel pela navega√ß√£o, requisi√ß√£o HTTP, extra√ß√£o de links e busca de keywords.Decis√µes T√©cnicas Chave:I/O N√£o-Bloqueante (HttpClient.sendAsync()): Esta √© a alma da performance. Em vez de bloquear uma thread por centenas de milissegundos esperando a resposta de um servidor, sendAsync() libera a thread imediatamente. O c√≥digo de processamento √© anexado via .thenAccept(), que √© executado apenas quando os dados chegam.Recurs√£o Ass√≠ncrona: A fun√ß√£o crawlAsync() se chama recursivamente para novos links, mas como cada chamada √© n√£o-bloqueante, ela n√£o estoura a pilha de chamadas e n√£o congestiona o sistema com espera de I/O.Controle Thread-Safe do Escopo: Usa um Set<String> baseado em ConcurrentHashMap (visitedUrls) para garantir que:Apenas URLs dentro da BASE_URL sejam seguidas (Requisito 4).Nenhuma URL seja visitada mais de uma vez, mesmo se m√∫ltiplos threads a encontrarem simultaneamente.Keyword Case Insensitive (R2): O termo √© compilado em um Pattern com a flag Pattern.CASE_INSENSITIVE para garantir a busca correta.3. CrawlJob.java (Modelo de Dados)Fun√ß√£o: Representa o estado e os resultados de uma √∫nica busca.Decis√µes T√©cnicas Chave:volatile String status: O status √© uma String (para compatibilidade simples com GSON/JSON) e √© marcado como volatile. Isso garante que qualquer thread que altere o status (ex: o crawler alterando para done) ter√° a mudan√ßa imediatamente vis√≠vel para qualquer outro thread (ex: o thread da API que consulta o status).Set<String> baseado em ConcurrentHashMap: Garante que a lista de URLs encontradas (urls) √© thread-safe e n√£o aceita duplicatas, sendo acess√≠vel simultaneamente por threads de escrita (crawler) e threads de leitura (API).4. Por que Escolhemos Esta Abordagem? (Vantagens)Abordagem EscolhidaAbordagem Alternativa (Bloqueante)VantagemI/O N√£o-Bloqueante (sendAsync)I/O Bloqueante (send)Performance e Escalabilidade: Uma √∫nica thread pode gerenciar milhares de conex√µes HTTP abertas e ativas, esperando a resposta do servidor.HttpClient CompartilhadoCriar um HttpClient por requisi√ß√£oLat√™ncia M√≠nima: Evita a sobrecarga de inicializar recursos internos de rede (pools e sockets) repetidamente.ConcurrentHashMapHashMap simples ou synchronized ListRobustez Concorrente: Permite alta taxa de acesso simult√¢neo por m√∫ltiplos threads sem corromper os dados e com bloqueios otimizados apenas nas partes necess√°rias.Arquitetura Spark + Java 11+Servlet Container tradicional (e.g., Tomcat)Simplicidade: O Spark e o HttpClient nativo oferecem uma solu√ß√£o leve, r√°pida e moderna, perfeita para um microsservi√ßo focado em performance.